{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-U3SI4eCAUTv"
   },
   "source": [
    "## Linear Regression and Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_w-yUAYN-w2M"
   },
   "source": [
    "# **Concepts with Linear Regression**\n",
    "Many of the important concepts in supervised machine learning can be appreciated by a solid understanding of linear regression, which we all probably encountered early on -- like elementary school. We are going to work through a linear regression task in a fashion  to appreciate many of the basic mechanics that underlie deep learning.\n",
    "\n",
    "To start, we will pull a dataset that explores the use of machine learning to predict polymer properties. The label  that we extract (and our target for prediction) will be the radius of gyration ($R_g$), which provides a measure of an object's size, for some simulated intrinsically disordered proteins.\n",
    "\n",
    "A well known result from polymer physics is that [$R_g \\propto M^{0.5}$](https://en.wikipedia.org/wiki/Ideal_chain), where $M$ is the number of statistically uncorrelated sub-units in the polymer; the same result arises in the context of diffusion/random walks.\n",
    "\n",
    "In the following, we will examine how well the data on $R_g$ can be described by a simple linear model with $N^{0.5}$ as our input feature ($N$ will be the number of residues in the protein, which we do not generally expect to be a statistically uncorrelated sub-unit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4057,
     "status": "ok",
     "timestamp": 1726166986237,
     "user": {
      "displayName": "Michael Webb",
      "userId": "16285727522458314478"
     },
     "user_tz": 240
    },
    "id": "r5vOo_ybAiva"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "import random\n",
    "from sklearn.metrics import r2_score, mean_squared_error,mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zc-bNguzAzsy"
   },
   "source": [
    "## **Grabbing data from a remote source online**\n",
    "\n",
    "In the following cell, we show how to grab some data in reasonable format from online. `pandas` provides a convenient way to parse `.csv` files. Simple text files can be processed using standard I/O commands.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 1072,
     "status": "ok",
     "timestamp": 1726166989189,
     "user": {
      "displayName": "Michael Webb",
      "userId": "16285727522458314478"
     },
     "user_tz": 240
    },
    "id": "FhDJJxhLAkM3",
    "outputId": "8130df32-6909-442f-830a-e00a1cc85642"
   },
   "outputs": [],
   "source": [
    "url_for_labels    = \"https://raw.githubusercontent.com/lsmin0152/cheb301/refs/heads/main/notebooks/data/labels.csv\"\n",
    "url_for_sequences = \"https://raw.githubusercontent.com/lsmin0152/cheb301/refs/heads/main/notebooks/data/sequences.txt\"\n",
    "idpdata = pd.read_csv(url_for_labels)\n",
    "\n",
    "y = idpdata['ROG (A)'].to_numpy()/10.     # these are now labels\n",
    "seqs  = [line.strip().split() for line in urllib.request.urlopen(url_for_sequences)]\n",
    "X     = np.array([len(seq) for seq in seqs])**0.5   # these are features\n",
    "\n",
    "idpdata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8OB5RPW3C2Hw"
   },
   "source": [
    "## **Plotting data**\n",
    "\n",
    "The cell below creates a function that will make a relatively standard but reasonably aesthetic plot. Because we are going to make a lot of these plots, we create a function instead of hard-coding the single instance, requiring many copy/pastes!\n",
    "\n",
    "Go ahead and play around with some options and customize it to your liking. You'll notice the use of `rcParams`, which provides access to a large number of configurable aspects of plots by default; you can view these [here](https://matplotlib.org/stable/tutorials/introductory/customizing.html).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 477
    },
    "executionInfo": {
     "elapsed": 1845,
     "status": "ok",
     "timestamp": 1726166994255,
     "user": {
      "displayName": "Michael Webb",
      "userId": "16285727522458314478"
     },
     "user_tz": 240
    },
    "id": "MllOQE8NCWY1",
    "outputId": "08c05e92-f113-4bab-915e-6f777424e41b"
   },
   "outputs": [],
   "source": [
    "# global specifications on plots\n",
    "plt.rcParams.update({'font.size': 18,\n",
    "                     'font.weight' : 'bold',\n",
    "                     'axes.labelweight': 'bold'})\n",
    "\n",
    "def plot_raw_data(x,y):\n",
    "  plt.plot(x, y,marker='o',linestyle=\"\",markersize=8,\\\n",
    "           color='r',markeredgecolor='k')\n",
    "  plt.ylabel(\"Radius of Gyration, $R_g$ (nm)\")\n",
    "  plt.xlabel(\"$N^{0.5}$\")\n",
    "  plt.xlim(0,30)\n",
    "  plt.ylim(0,10)\n",
    "  ax = plt.gca()\n",
    "  ax.tick_params(direction='in')\n",
    "  ax.yaxis.set_ticks_position('both')\n",
    "  ax.xaxis.set_ticks_position('both')\n",
    "  return ax\n",
    "\n",
    "ax = plot_raw_data(X,y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fHDVE2uAGFDb"
   },
   "source": [
    "## **Examining a human hypothesis**\n",
    "At first glance, it certainly seems that there is reasonable linear correlation between our input and labels.\n",
    "\n",
    "We are going to consider a function of the form\n",
    "\n",
    "$$R_g = \\theta_0 + \\theta_1 N^{0.5}$$\n",
    "\n",
    "Can you look at the plot to \"guess\" values of these parameters? Complete the cell below and explore some \"hypotheses\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "executionInfo": {
     "elapsed": 1035,
     "status": "ok",
     "timestamp": 1726167045475,
     "user": {
      "displayName": "Michael Webb",
      "userId": "16285727522458314478"
     },
     "user_tz": 240
    },
    "id": "UmcXkqRSCz9S",
    "outputId": "04d980ac-284c-4c0d-c40c-386c0329e0fe"
   },
   "outputs": [],
   "source": [
    "# basic set up\n",
    "Nmax = 900\n",
    "xline= np.array(range(Nmax+1))**0.5\n",
    "\n",
    "def f(x, th):\n",
    "    return th[0] + th[1]*x\n",
    "\n",
    "# fill in parameters\n",
    "thetas = np.array([[0.1,0.3]]).T # this maps to a 2x1\n",
    "\n",
    "# make predictions using function\n",
    "yline = f(xline,thetas)\n",
    "\n",
    "# examine hypothesis\n",
    "ax = plot_raw_data(X,y)\n",
    "ax.plot(xline,yline,color='y',linewidth=3,linestyle=':')\n",
    "plt.show()\n",
    "\n",
    "# make predictions from features and compute evaluation metrics\n",
    "yhat  = f(X,thetas)\n",
    "r2    = r2_score(y,yhat)\n",
    "rmse  = mean_squared_error(y,yhat)\n",
    "mae   = mean_absolute_error(y,yhat)\n",
    "print(\"r2 = {:>5.3f}, MSE = {:>5.3f}, MAE = {:>5.3f}\"\\\n",
    "      .format(r2,rmse,mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bcEHzD8TGetH"
   },
   "source": [
    "## **More formal optimization**\n",
    "Probably your human-intuited fit yields a pretty darn good description of the data, but we'll try to do better.\n",
    "\n",
    "The ``training'' of neural networks is really about *optimization* where the objective is to minimze a *loss* function that describes a disparity between the model predictions and the ground truth of some set of labels.\n",
    "\n",
    "When we first encounter linear regression, our optimization is usually in the `least-squares' sense; that is, our loss function is the mean-squared error over our observations. The problem of linear least-squares regression can be ``exactly'' solved using techniques from linear algebra.\n",
    "While that is not so much the domain of machine learning, let's see what answer we get via this approach to gain some expectation/intuition for the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m5pajYKYHdxK"
   },
   "source": [
    "## **Linear algebraic solution**\n",
    "If run for enough iterations, the solution obtained from gradient descent should outperform our human-intuited estimated model, even if just slightly. Because this is least-squares linear regression, we can also compare our solution with that obtained via linear algebra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 117,
     "status": "ok",
     "timestamp": 1726167099842,
     "user": {
      "displayName": "Michael Webb",
      "userId": "16285727522458314478"
     },
     "user_tz": 240
    },
    "id": "OsCQ6qLjHP44",
    "outputId": "956df975-c53d-4980-94e4-15ce18281da4"
   },
   "outputs": [],
   "source": [
    "N = len(y)\n",
    "M = 2\n",
    "A = np.ones((N,M))\n",
    "A[:,1] = X[:]\n",
    "thetaOpt = np.linalg.inv(A.T@A)@A.T@y # explicit solution for non-square matrix\n",
    "#thetaOpt  = np.linalg.pinv(A)@y       # hiding the math through pinv (Penrose Inverse)\n",
    "yhat  = f(X,thetaOpt)\n",
    "r2    = r2_score(y,yhat)\n",
    "mse   = mean_squared_error(y,yhat)\n",
    "mae   = mean_absolute_error(y,yhat)\n",
    "print(\"theta_0 = {:>8.4f}\".format(thetaOpt[0]))\n",
    "print(\"theta_1 = {:>8.4f}\".format(thetaOpt[1]))\n",
    "print(\"r2 = {:>5.3f}, MSE = {:>8.5f}, MAE = {:>5.3f}\"\\\n",
    "      .format(r2,rmse,mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yTsrXbw5kS1H"
   },
   "source": [
    "# **Optimization with a loss function**\n",
    "\n",
    "Now, we will approach a solution using gradient descent optimization, which is more akin to what is needed fro training a neural network.\n",
    "\n",
    "Below we define and examine our loss function (a mean-squared error) over our parameter space (considering the whole dataset). We also place a square at the position of the hypothesis and a star at the solution obtained using linear algebra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_rEow2rBGQdB"
   },
   "outputs": [],
   "source": [
    "def loss(x,y,theta):\n",
    "  ''' Function to calculate cost function assuming a hypothesis of form y^ = X*theta\n",
    "  Inputs:\n",
    "  x = array of dependent variable\n",
    "  y = array of training examples\n",
    "  theta = array of parameters for hypothesis\n",
    "\n",
    "  Returns:\n",
    "  E = cost function\n",
    "  '''\n",
    "  n        = len(y) #number of training examples\n",
    "  features = np.ones((n,len(theta))) # X\n",
    "  features[:,1] = x[:]\n",
    "  ypred = features@theta # predictions with current hypothesis\n",
    "  E = np.sum((ypred[:,0]-y[:])**2)/n #Cost function\n",
    "  return E\n",
    "\n",
    "def plot_loss(t0,t1):\n",
    "  #Initialize E as a matrix to store cost function values\n",
    "  E = np.zeros((len(t0),len(t1)))\n",
    "\n",
    "  # Populate matrix\n",
    "  for i,theta0 in enumerate(theta0s):\n",
    "    for j,theta1 in enumerate(theta1s):\n",
    "      theta_ij = np.array([[theta0,theta1]]).T\n",
    "      E[i,j]   = loss(X,y,theta_ij)\n",
    "  t0g,t1g = np.meshgrid(t0,t1)\n",
    "  fig = plt.figure(figsize=(15,4))\n",
    "  ax1  = fig.add_subplot(1,2,1,projection='3d')\n",
    "  surf = ax1.plot_surface(t0g, t1g, E, linewidth=0, antialiased=False,cmap='coolwarm')\n",
    "  ax1.set_xlabel(r\"$\\theta_1$\")\n",
    "  ax1.set_ylabel(r\"$\\theta_0$\")\n",
    "  ax1.set_zlabel(r\"$E$\")\n",
    "  ax2 = fig.add_subplot(1,2,2)\n",
    "  CS = ax2.contour(t0g,t1g,E.T,np.logspace(-3,2,25),cmap='coolwarm')\n",
    "  ax2.set_xlabel(r\"$\\theta_0$\")\n",
    "  ax2.set_ylabel(r\"$\\theta_1$\")\n",
    "\n",
    "  return fig,ax1,ax2\n",
    "\n",
    "#Define grid over which to calculate loss function\n",
    "N = 50\n",
    "theta0Rng = [-5,5]\n",
    "theta1Rng = [-0.5,1.5]\n",
    "theta0s = np.linspace(theta0Rng[0],theta0Rng[1],N)\n",
    "theta1s = np.linspace(theta1Rng[0],theta1Rng[1],N)\n",
    "\n",
    "fig,ax1,ax2 = plot_loss(theta0s,theta1s)\n",
    "ax2.plot(thetas[0],thetas[1],marker='s',color='m',markersize=10)\n",
    "ax2.plot(thetaOpt[0],thetaOpt[1],marker='*',color='y',markersize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cc-Ts9OXG8QJ"
   },
   "source": [
    "## **Implementation of Gradient Descent**\n",
    "\n",
    "Next, we will implement gradient descent to find an optimal set of parameters. For this type of linear model, it is possible to obtain the requisite derivative of the loss function with respect to the parameters analytically. I have used that solution below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rbj3cTdOHH64"
   },
   "outputs": [],
   "source": [
    "def E2loss(yhat,y):\n",
    "    return np.sum((np.squeeze(yhat)[:]-y[:])**2)/len(y)\n",
    "\n",
    "def Grad_Descent(x,y,theta,alpha,nIters,x_te=None,y_te=None):\n",
    "  '''Gradient descent algorithm\n",
    "  Inputs:\n",
    "  x = dependent variable\n",
    "  y = training data\n",
    "  theta = parameters\n",
    "  alpha = learning rate\n",
    "  iters = number of iterations\n",
    "  Output:\n",
    "  theta = final parameters\n",
    "  E = array of cost as a function of iterations\n",
    "  '''\n",
    "  n        = len(y) #number of training examples\n",
    "  features = np.ones((n,len(theta)))\n",
    "  features[:,1] = x[:]\n",
    "  yhat  = features@theta # predictions with current hypothesis\n",
    "  E_hist = [E2loss(yhat,y)]\n",
    "\n",
    "  if x_te is not None:\n",
    "    E_hist_te = [E2loss(f(x_te,theta),y_te)]\n",
    "\n",
    "  for i in range(nIters):\n",
    "    e     = yhat[:,0] - y[:]\n",
    "    theta = theta - (alpha*e[:,np.newaxis].T@features).T #\n",
    "    yhat = features@theta # predictions with current hypothesis\n",
    "    E_hist.append(E2loss(yhat,y))\n",
    "    if x_te is not None:\n",
    "      E_hist_te.append(E2loss(f(x_te,theta),y_te))\n",
    "\n",
    "  if x_te is not None:\n",
    "    return theta,E_hist,E_hist_te\n",
    "  else:\n",
    "    return theta,E_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Fb0DjFkmZR_"
   },
   "source": [
    "Let's go ahead and do the optimization. We are also specifying a value of a \"learning\" rate, which is a so-called hyperparmeter in our model training/optimization. Take a moment to interrogate and understand the sizes/shapes and objects that are being used in the `Grad_Descent` function. Print out the values of the final set of parameters with a formatted print statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7NPJZjqkHJrR"
   },
   "outputs": [],
   "source": [
    "th0     = np.array([[-1.],[.75]])\n",
    "nIters = 5000\n",
    "thetaGD, EGD = Grad_Descent(X,y,th0,8e-6,nIters)\n",
    "print(\"theta_0 = {:>8.4f}\".format(thetaGD[0,0]))\n",
    "print(\"theta_1 = {:>8.4f}\".format(thetaGD[1,0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0F8B2He2HMcG"
   },
   "source": [
    "The function also outputs the set of loss values observed over the course of the optimization. Let's plot those to monitor how things proceeded. After observing this \"training\" curve, consider going back and modifying the `alpha` parameter. How does that impact the model training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7AWuF-zEm-UE"
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "ax.plot(np.array(range(nIters+1))+1,np.array(EGD),linestyle='-',color = 'k',linewidth=3)\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "ax.set_xlabel(\"Iterations\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "plt.show()\n",
    "\n",
    "# examine solution\n",
    "ax = plot_raw_data(X,y)\n",
    "ax.plot(xline,f(xline,thetaGD),color='k',linewidth=3,linestyle=':')\n",
    "plt.show()\n",
    "r2    = r2_score(y,yhat)\n",
    "mse  = mean_squared_error(y,yhat)\n",
    "mae   = mean_absolute_error(y,yhat)\n",
    "print(\"r2 = {:>5.3f}, MSE = {:>8.5f}, MAE = {:>5.3f}\"\\\n",
    "      .format(r2,rmse,mae))\n",
    "\n",
    "\n",
    "fig,ax1,ax2 = plot_loss(theta0s,theta1s)\n",
    "ax2.plot(thetas[0],thetas[1],marker='s',color='m',markersize=10)\n",
    "ax2.plot(thetaOpt[0],thetaOpt[1],marker='*',color='m',markersize=20)\n",
    "ax2.plot(thetaGD[0],thetaGD[1],marker='*',color='y',markersize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-nW8_TyUHk4Z"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Bo1mjyB-w2Q"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
